{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "An implementation of the policyValueNet in PyTorch\n",
    "Tested in PyTorch 0.2.0 and 0.3.0\n",
    "@author: Junxiao Song\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    \"\"\"Sets the learning rate to the given value\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"policy-value network module\"\"\"\n",
    "    def __init__(self, board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        # common layers\n",
    "        self.conv1 = nn.Conv2d(11, 88, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(88, 176, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(176, 352, kernel_size=3, padding=1)\n",
    "        # action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(352, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*board_width*board_height,\n",
    "                                 board_width*board_height)\n",
    "        # state value layers\n",
    "        self.val_conv1 = nn.Conv2d(352, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2*board_width*board_height, 88)\n",
    "        self.val_fc2 = nn.Linear(88, 1)\n",
    "\n",
    "    def forward(self, state_input):\n",
    "        # common layers\n",
    "        x = F.relu(self.conv1(state_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # action policy layers\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        # state value layers\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "        return x_act, x_val\n",
    "\n",
    "\n",
    "class PolicyValueNet():\n",
    "    \"\"\"policy-value network \"\"\"\n",
    "    def __init__(self, board_width, board_height,\n",
    "                 model_file=None, use_gpu=False):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-4  # coef of l2 penalty\n",
    "        # the policy value net module\n",
    "        if self.use_gpu:\n",
    "            self.policy_value_net = Net(board_width, board_height).cuda()\n",
    "        else:\n",
    "            self.policy_value_net = Net(board_width, board_height)\n",
    "        self.optimizer = optim.Adam(self.policy_value_net.parameters(),\n",
    "                                    weight_decay=self.l2_const)\n",
    "\n",
    "        if model_file:\n",
    "            net_params = pickle.load(open(model_file, 'rb'))\n",
    "            self.policy_value_net.load_state_dict(net_params)\n",
    "\n",
    "    def policy_value(self, state_batch):\n",
    "        \"\"\"\n",
    "        input: a batch of states\n",
    "        output: a batch of action probabilities and state values\n",
    "        \"\"\"\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
    "            return act_probs, value.data.cpu().numpy()\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.numpy())\n",
    "            return act_probs, value.data.numpy()\n",
    "\n",
    "    def policy_value_fn(self, board):\n",
    "        \"\"\"\n",
    "        input: board\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "        action and the score of the board state\n",
    "        \"\"\"\n",
    "        legal_positions = board.availables\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
    "                -1, 11, self.board_width, self.board_height))\n",
    "        if self.use_gpu:\n",
    "            log_act_probs, value = self.policy_value_net(\n",
    "                    Variable(torch.from_numpy(current_state)).cuda().float())\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
    "        else:\n",
    "            log_act_probs, value = self.policy_value_net(\n",
    "                    Variable(torch.from_numpy(current_state)).float())\n",
    "            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        value = value.data[0][0]\n",
    "        return act_probs, value\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
    "        \"\"\"perform a training step\"\"\"\n",
    "        # wrap in Variable\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch))\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # set learning rate\n",
    "        set_learning_rate(self.optimizer, lr)\n",
    "\n",
    "        # forward\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        # Note: the L2 penalty is incorporated in optimizer\n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # calc policy entropy, for monitoring only\n",
    "        entropy = -torch.mean(\n",
    "                torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)\n",
    "                )\n",
    "        return loss.data[0], entropy.data[0]\n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\" save model params to file \"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
